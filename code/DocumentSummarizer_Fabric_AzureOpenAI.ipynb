{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50c63ff4",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0b9c9d4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-11-26T04:15:28.407248Z",
       "execution_start_time": "2025-11-26T04:15:12.9525232Z",
       "normalized_state": "finished",
       "parent_msg_id": "5b78c92f-cae8-44ec-99ac-f34a0e6b93f2",
       "queued_time": "2025-11-26T04:15:07.2205047Z",
       "session_id": "9f63340c-66f4-4ff0-a5fb-e83f404be2ff",
       "session_start_time": "2025-11-26T04:15:07.2213586Z"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Installing dependencies...\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "âœ… Packages installed!\n"
     ]
    }
   ],
   "source": [
    "# === PACKAGE INSTALLATION ===\n",
    "print(\"ðŸ“¦ Installing dependencies...\")\n",
    "\n",
    "# Core packages for document processing\n",
    "%pip install requests python-docx PyMuPDF Pillow --quiet\n",
    "\n",
    "# HTTP client for API calls\n",
    "%pip install httpx --quiet\n",
    "\n",
    "# OpenTelemetry for tracing (optional)\n",
    "%pip install opentelemetry-sdk --quiet\n",
    "\n",
    "print(\"âœ… Packages installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b353d1d3",
   "metadata": {},
   "source": [
    "## 2. Setup Fabric OpenAI Configuration\n",
    "\n",
    "This uses Fabric's built-in authentication - **no API keys needed!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6c2661f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-11-26T04:15:37.7914371Z",
       "execution_start_time": "2025-11-26T04:15:30.3753382Z",
       "normalized_state": "finished",
       "parent_msg_id": "c11e1de5-c6d6-4dad-b2f2-e4bf4e242988",
       "queued_time": "2025-11-26T04:15:30.3743412Z",
       "session_id": "9f63340c-66f4-4ff0-a5fb-e83f404be2ff",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Setting up Fabric OpenAI configuration...\n",
      "âœ… Fabric OpenAI configuration successful!\n",
      "ðŸ¤– Model: gpt-5\n",
      "ðŸ”— Endpoint configured\n",
      "ðŸ” Authentication: Fabric managed (no API key needed)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Fabric's built-in authentication\n",
    "from synapse.ml.fabric.service_discovery import get_fabric_env_config\n",
    "from synapse.ml.fabric.token_utils import TokenUtils\n",
    "\n",
    "# ===================== CONFIGURATION =====================\n",
    "# Only these parameters need to be configured:\n",
    "\n",
    "# GPT Model Configuration (Fabric built-in)\n",
    "DEPLOYMENT_NAME = \"gpt-5\"  # Update to match your Fabric deployment\n",
    "API_VERSION = \"2024-08-01-preview\"\n",
    "# Note: GPT-5 doesn't support max_tokens or temperature parameters\n",
    "\n",
    "# SharePoint Repository Configuration\n",
    "SHAREPOINT_DOCUMENT_ROOT = \"https://mngenvmcap470378.sharepoint.com/sites/Tobedeleted/Shared%20Documents/\"\n",
    "# This is the direct document root URL where files are stored\n",
    "\n",
    "# ===================== FABRIC OPENAI SETUP =====================\n",
    "print(\"ðŸ”§ Setting up Fabric OpenAI configuration...\")\n",
    "\n",
    "def get_fabric_openai_config():\n",
    "    \"\"\"\n",
    "    Get OpenAI configuration from Fabric environment.\n",
    "    Returns the service URL and authentication headers.\n",
    "    \n",
    "    No API keys needed - Fabric handles authentication automatically!\n",
    "    \"\"\"\n",
    "    fabric_env_config = get_fabric_env_config().fabric_env_config\n",
    "    auth_header = TokenUtils().get_openai_auth_header()\n",
    "    \n",
    "    openai_base_host = fabric_env_config.ml_workload_endpoint + \"cognitive/openai/openai/\"\n",
    "    service_url = f\"{openai_base_host}deployments/{DEPLOYMENT_NAME}/chat/completions?api-version={API_VERSION}\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": auth_header,\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    return service_url, headers\n",
    "\n",
    "# Test the configuration\n",
    "try:\n",
    "    service_url, auth_headers = get_fabric_openai_config()\n",
    "    print(\"âœ… Fabric OpenAI configuration successful!\")\n",
    "    print(f\"ðŸ¤– Model: {DEPLOYMENT_NAME}\")\n",
    "    print(f\"ðŸ”— Endpoint configured\")\n",
    "    print(f\"ðŸ” Authentication: Fabric managed (no API key needed)\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error setting up Fabric OpenAI: {e}\")\n",
    "    print(\"ðŸ’¡ Make sure you're running this in a Fabric notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fe1d60",
   "metadata": {},
   "source": [
    "## 3. Ready to Analyze!\n",
    "\n",
    "All configuration complete. The multimodal approach uses GPT-5's vision capabilities directly - no external services needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e220b4f1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-11-26T04:15:51.0017051Z",
       "execution_start_time": "2025-11-26T04:15:50.5869869Z",
       "normalized_state": "finished",
       "parent_msg_id": "4b2d05c1-ffcf-4f46-b7d2-a437d201787d",
       "queued_time": "2025-11-26T04:15:50.5859966Z",
       "session_id": "9f63340c-66f4-4ff0-a5fb-e83f404be2ff",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Configuration complete!\n",
      "ðŸ’¡ Using GPT-5's multimodal capabilities:\n",
      "   - PDF documents: Converted to images for vision analysis\n",
      "   - DOCX documents: Text extraction or image conversion\n",
      "   - Images: Direct analysis with GPT-5 Vision\n",
      "   - No external APIs required!\n"
     ]
    }
   ],
   "source": [
    "# === MULTIMODAL PROCESSING READY ===\n",
    "# No external services needed!\n",
    "# GPT-5 will analyze documents directly using its vision capabilities\n",
    "\n",
    "print(\"âœ… Configuration complete!\")\n",
    "print(\"ðŸ’¡ Using GPT-5's multimodal capabilities:\")\n",
    "print(\"   - PDF documents: Converted to images for vision analysis\")\n",
    "print(\"   - DOCX documents: Text extraction or image conversion\")\n",
    "print(\"   - Images: Direct analysis with GPT-5 Vision\")\n",
    "print(\"   - No external APIs required!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34136124",
   "metadata": {},
   "source": [
    "## 4. Batch Processing Setup\n",
    "\n",
    "Configure state management and document discovery for automated batch processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d6bd94",
   "metadata": {},
   "source": [
    "### 4.1 Processing State Manager\n",
    "\n",
    "Track processed files to avoid reprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062d0f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from typing import Set, Dict\n",
    "\n",
    "class ProcessingStateManager:\n",
    "    \"\"\"\n",
    "    Manages state of processed documents to avoid reprocessing.\n",
    "    Tracks file hash, processing date, and status.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_file_path: str = \"/lakehouse/default/Files/Document_Summaries/DocProcessingStateManager/.processing_state.json\"):\n",
    "        self.state_file_path = state_file_path\n",
    "        self.state = self._load_state()\n",
    "    \n",
    "    def _load_state(self) -> Dict:\n",
    "        \"\"\"Load processing state from JSON file.\"\"\"\n",
    "        if os.path.exists(self.state_file_path):\n",
    "            try:\n",
    "                with open(self.state_file_path, 'r', encoding='utf-8') as f:\n",
    "                    return json.load(f)\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸  Could not load state file: {e}\")\n",
    "                return {\"processed_files\": {}, \"metadata\": {\"version\": \"1.0\"}}\n",
    "        else:\n",
    "            return {\"processed_files\": {}, \"metadata\": {\"version\": \"1.0\"}}\n",
    "    \n",
    "    def _save_state(self):\n",
    "        \"\"\"Save processing state to JSON file.\"\"\"\n",
    "        try:\n",
    "            # Ensure directory exists\n",
    "            os.makedirs(os.path.dirname(self.state_file_path), exist_ok=True)\n",
    "            \n",
    "            with open(self.state_file_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(self.state, f, indent=2, ensure_ascii=False)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  Could not save state file: {e}\")\n",
    "    \n",
    "    def _calculate_file_hash(self, file_path: str) -> str:\n",
    "        \"\"\"Calculate SHA256 hash of file content.\"\"\"\n",
    "        sha256_hash = hashlib.sha256()\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            # Read file in chunks to handle large files\n",
    "            for byte_block in iter(lambda: f.read(4096), b\"\"):\n",
    "                sha256_hash.update(byte_block)\n",
    "        return sha256_hash.hexdigest()\n",
    "    \n",
    "    def is_processed(self, file_path: str) -> bool:\n",
    "        \"\"\"\n",
    "        Check if file has been processed.\n",
    "        Returns True if file exists in state with same hash.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(file_path):\n",
    "            return False\n",
    "        \n",
    "        file_hash = self._calculate_file_hash(file_path)\n",
    "        file_name = os.path.basename(file_path)\n",
    "        \n",
    "        if file_name in self.state[\"processed_files\"]:\n",
    "            stored_hash = self.state[\"processed_files\"][file_name].get(\"file_hash\")\n",
    "            return stored_hash == file_hash\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def mark_processed(self, file_path: str, summary_path: str = None, json_path: str = None, status: str = \"success\"):\n",
    "        \"\"\"\n",
    "        Mark file as processed with metadata.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to processed document\n",
    "            summary_path: Path to generated summary\n",
    "            json_path: Path to generated JSON\n",
    "            status: Processing status (success/failed)\n",
    "        \"\"\"\n",
    "        file_hash = self._calculate_file_hash(file_path)\n",
    "        file_name = os.path.basename(file_path)\n",
    "        \n",
    "        self.state[\"processed_files\"][file_name] = {\n",
    "            \"file_hash\": file_hash,\n",
    "            \"file_path\": file_path,\n",
    "            \"processed_date\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            \"summary_output\": summary_path,\n",
    "            \"json_output\": json_path,\n",
    "            \"status\": status,\n",
    "            \"file_size_kb\": os.path.getsize(file_path) / 1024\n",
    "        }\n",
    "        \n",
    "        self._save_state()\n",
    "    \n",
    "    def mark_failed(self, file_path: str, error_message: str):\n",
    "        \"\"\"Mark file as failed with error message.\"\"\"\n",
    "        file_name = os.path.basename(file_path)\n",
    "        \n",
    "        self.state[\"processed_files\"][file_name] = {\n",
    "            \"file_hash\": self._calculate_file_hash(file_path) if os.path.exists(file_path) else None,\n",
    "            \"file_path\": file_path,\n",
    "            \"processed_date\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            \"status\": \"failed\",\n",
    "            \"error\": error_message\n",
    "        }\n",
    "        \n",
    "        self._save_state()\n",
    "    \n",
    "    def get_processed_count(self) -> int:\n",
    "        \"\"\"Get count of successfully processed files.\"\"\"\n",
    "        return sum(1 for f in self.state[\"processed_files\"].values() if f.get(\"status\") == \"success\")\n",
    "    \n",
    "    def get_failed_count(self) -> int:\n",
    "        \"\"\"Get count of failed files.\"\"\"\n",
    "        return sum(1 for f in self.state[\"processed_files\"].values() if f.get(\"status\") == \"failed\")\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset all processing state (use with caution!).\"\"\"\n",
    "        self.state = {\"processed_files\": {}, \"metadata\": {\"version\": \"1.0\"}}\n",
    "        self._save_state()\n",
    "        print(\"âœ… Processing state reset\")\n",
    "\n",
    "\n",
    "# Initialize state manager\n",
    "state_manager = ProcessingStateManager()\n",
    "\n",
    "print(\"âœ… Processing State Manager initialized\")\n",
    "print(f\"ðŸ“Š Previously processed: {state_manager.get_processed_count()} files\")\n",
    "print(f\"âŒ Failed: {state_manager.get_failed_count()} files\")\n",
    "print(f\"ðŸ’¾ State file: {state_manager.state_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c491c8",
   "metadata": {},
   "source": [
    "### 4.2 Document Discovery\n",
    "\n",
    "Find all documents in the lakehouse SharePointDocuments directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deeea4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from typing import List\n",
    "\n",
    "def discover_documents(search_pattern: str = \"Files/SharePointDocuments/**/*\", \n",
    "                       supported_extensions: List[str] = ['.pdf', '.docx', '.txt', '.md']) -> List[str]:\n",
    "    \"\"\"\n",
    "    Discover all documents in the lakehouse SharePointDocuments directory.\n",
    "    \n",
    "    Args:\n",
    "        search_pattern: Glob pattern to search (relative to lakehouse root)\n",
    "        supported_extensions: List of file extensions to include\n",
    "    \n",
    "    Returns:\n",
    "        List of absolute paths to discovered documents\n",
    "    \"\"\"\n",
    "    lakehouse_path = \"/lakehouse/default\"\n",
    "    search_path = os.path.join(lakehouse_path, search_pattern)\n",
    "    \n",
    "    print(f\"ðŸ” Searching for documents...\")\n",
    "    print(f\"   Pattern: {search_path}\")\n",
    "    print(f\"   Extensions: {', '.join(supported_extensions)}\")\n",
    "    \n",
    "    # Find all files matching pattern\n",
    "    all_files = glob.glob(search_path, recursive=True)\n",
    "    \n",
    "    # Filter by extension and exclude hidden/system files\n",
    "    documents = [\n",
    "        f for f in all_files \n",
    "        if os.path.isfile(f) \n",
    "        and any(f.lower().endswith(ext) for ext in supported_extensions)\n",
    "        and not os.path.basename(f).startswith('.')\n",
    "        and not os.path.basename(f).startswith('~')\n",
    "    ]\n",
    "    \n",
    "    print(f\"âœ… Found {len(documents)} document(s)\")\n",
    "    \n",
    "    return documents\n",
    "\n",
    "\n",
    "def get_unprocessed_documents(search_pattern: str = \"Files/SharePointDocuments/**/*\") -> List[str]:\n",
    "    \"\"\"\n",
    "    Get list of documents that haven't been processed yet.\n",
    "    \n",
    "    Returns:\n",
    "        List of absolute paths to unprocessed documents\n",
    "    \"\"\"\n",
    "    all_documents = discover_documents(search_pattern)\n",
    "    \n",
    "    unprocessed = []\n",
    "    for doc_path in all_documents:\n",
    "        if not state_manager.is_processed(doc_path):\n",
    "            unprocessed.append(doc_path)\n",
    "    \n",
    "    print(f\"\\nðŸ“‹ Processing Summary:\")\n",
    "    print(f\"   Total documents: {len(all_documents)}\")\n",
    "    print(f\"   Already processed: {len(all_documents) - len(unprocessed)}\")\n",
    "    print(f\"   New/modified: {len(unprocessed)}\")\n",
    "    \n",
    "    return unprocessed\n",
    "\n",
    "\n",
    "# Discover documents\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸ“ DOCUMENT DISCOVERY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Option 1: Discover all documents in Files directory\n",
    "all_docs = discover_documents()\n",
    "\n",
    "# Option 2: Get only unprocessed documents\n",
    "unprocessed_docs = get_unprocessed_documents()\n",
    "\n",
    "if unprocessed_docs:\n",
    "    print(\"\\nðŸ†• Unprocessed documents:\")\n",
    "    for doc in unprocessed_docs:\n",
    "        file_size = os.path.getsize(doc) / 1024\n",
    "        print(f\"   - {os.path.basename(doc)} ({file_size:.1f} KB)\")\n",
    "else:\n",
    "    print(\"\\nâœ… All documents have been processed!\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae41876",
   "metadata": {},
   "source": [
    "## 5. Helper Functions\n",
    "\n",
    "Document processing functions and agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb575f4c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-11-26T04:15:59.6214437Z",
       "execution_start_time": "2025-11-26T04:15:58.756726Z",
       "normalized_state": "finished",
       "parent_msg_id": "cef3605a-93ab-4c5a-b38e-accc0c34df95",
       "queued_time": "2025-11-26T04:15:56.6004937Z",
       "session_id": "9f63340c-66f4-4ff0-a5fb-e83f404be2ff",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Helper functions defined\n",
      "ðŸ’¡ Using custom FabricAgent with multimodal capabilities!\n"
     ]
    }
   ],
   "source": [
    "import httpx\n",
    "import base64\n",
    "from typing import Dict, Any, Tuple, Optional\n",
    "\n",
    "class FabricAgent:\n",
    "    \"\"\"\n",
    "    Custom agent that works with Fabric's OpenAI endpoint.\n",
    "    Compatible with Microsoft Agent Framework interface.\n",
    "    Supports multimodal inputs (text + images).\n",
    "    \"\"\"\n",
    "    def __init__(self, deployment_name: str, api_version: str, instructions: str, name: str = \"FabricAgent\"):\n",
    "        self.deployment_name = deployment_name\n",
    "        self.api_version = api_version\n",
    "        self.instructions = instructions\n",
    "        self.name = name\n",
    "        self._service_url = None\n",
    "        self._headers = None\n",
    "    \n",
    "    def _ensure_config(self):\n",
    "        \"\"\"Get Fabric configuration.\"\"\"\n",
    "        if self._service_url is None:\n",
    "            fabric_env_config = get_fabric_env_config().fabric_env_config\n",
    "            auth_header = TokenUtils().get_openai_auth_header()\n",
    "            \n",
    "            openai_base_host = fabric_env_config.ml_workload_endpoint + \"cognitive/openai/openai/\"\n",
    "            self._service_url = f\"{openai_base_host}deployments/{self.deployment_name}/chat/completions?api-version={self.api_version}\"\n",
    "            \n",
    "            self._headers = {\n",
    "                \"Authorization\": auth_header,\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            }\n",
    "    \n",
    "    async def run(self, user_message: str, images: list = None):\n",
    "        \"\"\"\n",
    "        Run the agent with a user message and optional images.\n",
    "        Compatible with agent_framework.Agent.run() interface.\n",
    "        \n",
    "        Args:\n",
    "            user_message: Text prompt for the agent\n",
    "            images: Optional list of image data (base64 encoded or file paths)\n",
    "        \"\"\"\n",
    "        self._ensure_config()\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.instructions}\n",
    "        ]\n",
    "        \n",
    "        # Build user message with multimodal content if images provided\n",
    "        if images and len(images) > 0:\n",
    "            content = [{\"type\": \"text\", \"text\": user_message}]\n",
    "            for img in images:\n",
    "                content.append({\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": img}\n",
    "                })\n",
    "            messages.append({\"role\": \"user\", \"content\": content})\n",
    "        else:\n",
    "            messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "        \n",
    "        payload = {\"messages\": messages}\n",
    "        \n",
    "        async with httpx.AsyncClient(timeout=180.0) as client:\n",
    "            response = await client.post(\n",
    "                self._service_url,\n",
    "                headers=self._headers,\n",
    "                json=payload\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                return AgentResult(\n",
    "                    text=result[\"choices\"][0][\"message\"][\"content\"],\n",
    "                    raw_response=result\n",
    "                )\n",
    "            else:\n",
    "                raise Exception(f\"API Error: {response.status_code} - {response.text}\")\n",
    "\n",
    "\n",
    "class AgentResult:\n",
    "    \"\"\"Result object with .text property - compatible with agent_framework.\"\"\"\n",
    "    def __init__(self, text: str, raw_response: dict = None):\n",
    "        self.text = text\n",
    "        self.raw_response = raw_response\n",
    "\n",
    "\n",
    "def encode_image_to_base64(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Read an image file and encode it to base64 data URL.\n",
    "    Supports common image formats (PNG, JPEG, etc.)\n",
    "    \"\"\"\n",
    "    extension = Path(file_path).suffix.lower()\n",
    "    \n",
    "    mime_types = {\n",
    "        '.png': 'image/png',\n",
    "        '.jpg': 'image/jpeg',\n",
    "        '.jpeg': 'image/jpeg',\n",
    "        '.gif': 'image/gif',\n",
    "        '.webp': 'image/webp'\n",
    "    }\n",
    "    \n",
    "    mime_type = mime_types.get(extension, 'image/png')\n",
    "    \n",
    "    with open(file_path, 'rb') as f:\n",
    "        image_data = base64.b64encode(f.read()).decode('utf-8')\n",
    "    \n",
    "    return f\"data:{mime_type};base64,{image_data}\"\n",
    "\n",
    "\n",
    "def document_to_images(document_path: str, max_dimension: int = 1024, quality: int = 85) -> Tuple[list, Optional[str]]:\n",
    "    \"\"\"\n",
    "    Convert a document (PDF, DOCX) to images for multimodal processing.\n",
    "    Compresses images to avoid 413 payload too large errors.\n",
    "    \n",
    "    Args:\n",
    "        document_path: Path to document\n",
    "        max_dimension: Maximum width/height in pixels (default 1024 for API limits)\n",
    "        quality: JPEG quality 1-100 (default 85 balances size/quality)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (list of base64-encoded image data URLs, text content if DOCX)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import fitz  # PyMuPDF\n",
    "        from PIL import Image\n",
    "        import io\n",
    "        import zipfile\n",
    "        \n",
    "        # Verify file exists first\n",
    "        if not os.path.exists(document_path):\n",
    "            raise FileNotFoundError(f\"Document not found: {document_path}\")\n",
    "        \n",
    "        file_path_obj = Path(document_path)\n",
    "        file_extension = file_path_obj.suffix.lower()\n",
    "        \n",
    "        print(f\"   Processing file: {file_path_obj.name}\")\n",
    "        print(f\"   File type: {file_extension}\")\n",
    "        print(f\"   File size: {os.path.getsize(document_path) / 1024:.1f} KB\")\n",
    "        \n",
    "        images = []\n",
    "        \n",
    "        if file_extension == '.pdf':\n",
    "            # Convert PDF pages to images with compression\n",
    "            pdf_document = fitz.open(document_path)\n",
    "            \n",
    "            for page_num in range(len(pdf_document)):\n",
    "                page = pdf_document[page_num]\n",
    "                \n",
    "                # Render at lower resolution to reduce size (1.5x instead of 2x)\n",
    "                pix = page.get_pixmap(matrix=fitz.Matrix(1.5, 1.5))\n",
    "                \n",
    "                # Convert to PIL Image for compression\n",
    "                img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "                \n",
    "                # Resize if too large\n",
    "                if img.width > max_dimension or img.height > max_dimension:\n",
    "                    img.thumbnail((max_dimension, max_dimension), Image.Resampling.LANCZOS)\n",
    "                \n",
    "                # Compress to JPEG\n",
    "                buffer = io.BytesIO()\n",
    "                img.save(buffer, format='JPEG', quality=quality, optimize=True)\n",
    "                img_data = buffer.getvalue()\n",
    "                \n",
    "                # Encode to base64\n",
    "                img_base64 = base64.b64encode(img_data).decode('utf-8')\n",
    "                images.append(f\"data:image/jpeg;base64,{img_base64}\")\n",
    "                \n",
    "                print(f\"   Page {page_num + 1}: {len(img_base64) / 1024:.1f} KB\")\n",
    "            \n",
    "            pdf_document.close()\n",
    "            return images, None\n",
    "            \n",
    "        elif file_extension == '.docx':\n",
    "            # For DOCX, extract text directly here\n",
    "            print(\"   DOCX detected - extracting text content...\")\n",
    "            \n",
    "            # First check if it's a valid ZIP file (DOCX files are ZIP archives)\n",
    "            try:\n",
    "                with zipfile.ZipFile(document_path, 'r') as zip_ref:\n",
    "                    print(f\"   âœ… Valid DOCX structure detected\")\n",
    "            except zipfile.BadZipFile:\n",
    "                print(\"   âš ï¸  File is not a valid DOCX (not a ZIP file)\")\n",
    "                print(\"   ðŸ’¡ This might be a plain text file with .docx extension\")\n",
    "                print(\"   Attempting to read as plain text...\")\n",
    "                \n",
    "                # Try reading as plain text\n",
    "                with open(document_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    text_content = f.read()\n",
    "                \n",
    "                if text_content.strip():\n",
    "                    print(f\"   Extracted {len(text_content):,} characters as plain text\")\n",
    "                    return [], text_content\n",
    "                else:\n",
    "                    raise ValueError(\"File appears to be corrupted or empty\")\n",
    "            \n",
    "            # If we get here, it's a valid DOCX file\n",
    "            from docx import Document\n",
    "            \n",
    "            # Read file as binary first to avoid path encoding issues\n",
    "            with open(document_path, 'rb') as f:\n",
    "                doc = Document(f)\n",
    "            \n",
    "            text_content = \"\\n\".join([para.text for para in doc.paragraphs if para.text.strip()])\n",
    "            print(f\"   Extracted {len(text_content):,} characters from {len(doc.paragraphs)} paragraphs\")\n",
    "            \n",
    "            # Return empty images list and text content\n",
    "            return [], text_content\n",
    "            \n",
    "        elif file_extension in ['.png', '.jpg', '.jpeg', '.gif', '.webp']:\n",
    "            # Already an image - compress if needed\n",
    "            img = Image.open(document_path)\n",
    "            \n",
    "            # Resize if too large\n",
    "            if img.width > max_dimension or img.height > max_dimension:\n",
    "                img.thumbnail((max_dimension, max_dimension), Image.Resampling.LANCZOS)\n",
    "            \n",
    "            # Convert to RGB if needed\n",
    "            if img.mode != 'RGB':\n",
    "                img = img.convert('RGB')\n",
    "            \n",
    "            # Compress\n",
    "            buffer = io.BytesIO()\n",
    "            img.save(buffer, format='JPEG', quality=quality, optimize=True)\n",
    "            img_data = buffer.getvalue()\n",
    "            \n",
    "            img_base64 = base64.b64encode(img_data).decode('utf-8')\n",
    "            images.append(f\"data:image/jpeg;base64,{img_base64}\")\n",
    "            \n",
    "            print(f\"   Image compressed: {len(img_base64) / 1024:.1f} KB\")\n",
    "            return images, None\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file type: {file_extension}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error processing document: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return [], None\n",
    "\n",
    "\n",
    "print(\"âœ… Helper functions defined\")\n",
    "print(\"ðŸ’¡ Using custom FabricAgent with multimodal capabilities!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6371bbde",
   "metadata": {},
   "source": [
    "## 6. Document Summarization Prompts\n",
    "\n",
    "Configure AI agent instructions for analysis and extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42229c6b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-11-26T04:28:29.6368389Z",
       "execution_start_time": "2025-11-26T04:28:29.1975384Z",
       "normalized_state": "finished",
       "parent_msg_id": "287b50ea-3725-4d24-8caa-c879821c8c51",
       "queued_time": "2025-11-26T04:28:29.1967218Z",
       "session_id": "9f63340c-66f4-4ff0-a5fb-e83f404be2ff",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Agent prompts configured\n",
      "ðŸ“ Document Analyzer prompt: 720 characters\n",
      "ðŸ“ JSON Extractor prompt: 1,658 characters\n"
     ]
    }
   ],
   "source": [
    "# ===================== SUMMARIZATION PROMPTS =====================\n",
    "\n",
    "# Agent 1: Document Analyzer (creates summary from visual/multimodal analysis)\n",
    "DOCUMENT_ANALYZER_PROMPT = \"\"\"You are an advanced document analysis agent with multimodal capabilities.\n",
    "\n",
    "**Your Task:**\n",
    "Analyze the provided document images/content and create a comprehensive executive summary.\n",
    "\n",
    "**Instructions:**\n",
    "1. Carefully examine all pages of the document\n",
    "2. Identify the document type and structure\n",
    "3. Extract key information, themes, and insights\n",
    "4. Create a concise executive summary suitable for board-level presentation\n",
    "5. Focus on business impact and strategic implications\n",
    "\n",
    "**Output Format:**\n",
    "Provide a clear, structured summary in markdown format with:\n",
    "- Document title and type\n",
    "- Key findings (bullet points)\n",
    "- Executive summary (2-3 paragraphs)\n",
    "- Critical takeaways\n",
    "\n",
    "Be thorough but concise. Focus on what matters most.\"\"\"\n",
    "\n",
    "\n",
    "# Agent 2: JSON Extractor (extracts structured data)\n",
    "JSON_EXTRACTOR_PROMPT = \"\"\"You are a data extraction specialist.\n",
    "\n",
    "**Your Task:**\n",
    "Extract structured information from the document and return it as valid JSON.\n",
    "\n",
    "**Required Fields:**\n",
    "- document_title: The main title or subject of the document\n",
    "- document_type: Type of document (e.g., \"Regulatory Report\", \"Financial Statement\", \"Risk Assessment\")\n",
    "- document_date: Date mentioned in the document (if available, format as YYYY-MM-DD)\n",
    "- key_topics: String of main topics/themes discussed\n",
    "- critical_risks: String of identified risks or concerns comma separated\n",
    "- action_items: String of action items or recommendations comma separated\n",
    "- stakeholders: String of mentioned stakeholders or entities comma separated\n",
    "- risk_rating: Overall risk assessment (Low/Medium/High/Critical) if applicable\n",
    "- summary: Brief 1-2 sentence summary\n",
    "- author: Document author or department if mentioned\n",
    "- confidentiality: Confidentiality level if mentioned (e.g., \"Public\", \"Internal\", \"Confidential\", \"Restricted\")\n",
    "\n",
    "**Output Format:**\n",
    "Return ONLY valid JSON with the above structure. Do not include markdown formatting or code blocks.\n",
    "\n",
    "Example:\n",
    "{\n",
    "  \"document_title\": \"Q4 Risk Report\",\n",
    "  \"document_type\": \"Regulatory Risk Report\",\n",
    "  \"document_date\": \"2024-12-31\",\n",
    "  \"key_topics\": \"Remediation Programs\", \"Regulatory Engagement\",\n",
    "  \"critical_risks\": \"Compliance breach in X area\",\n",
    "  \"action_items\": \"Review policy Y\", \"Implement framework Z\",\n",
    "  \"stakeholders\": \"Executives\", \"Board of Directors\",\n",
    "  \"risk_rating\": \"Medium\",\n",
    "  \"summary\": \"Quarterly regulatory risk report covering remediation programs and regulatory engagement activities.\",\n",
    "  \"author\": \"Risk Management Group\",\n",
    "  \"confidentiality\": \"Internal\"\n",
    "}\"\"\"\n",
    "\n",
    "print(\"âœ… Agent prompts configured\")\n",
    "print(f\"ðŸ“ Document Analyzer prompt: {len(DOCUMENT_ANALYZER_PROMPT):,} characters\")\n",
    "print(f\"ðŸ“ JSON Extractor prompt: {len(JSON_EXTRACTOR_PROMPT):,} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8945f3e",
   "metadata": {},
   "source": [
    "## 7. Batch Processing Execution\n",
    "\n",
    "Automated processing of all unprocessed documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61f5c1e",
   "metadata": {},
   "source": [
    "### 7.1 Batch Processing Functions\n",
    "\n",
    "Process multiple documents with state tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9450f569",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_document_batch(document_paths: List[str], max_failures: int = 5):\n",
    "    \"\"\"\n",
    "    Process multiple documents in batch with state tracking.\n",
    "    Skips already-processed files and tracks failures.\n",
    "    \n",
    "    Args:\n",
    "        document_paths: List of document paths to process\n",
    "        max_failures: Stop batch if this many consecutive failures occur\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"ðŸ“¦ BATCH DOCUMENT PROCESSING\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total documents to process: {len(document_paths)}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    processed_count = 0\n",
    "    skipped_count = 0\n",
    "    failed_count = 0\n",
    "    consecutive_failures = 0\n",
    "    \n",
    "    for idx, doc_path in enumerate(document_paths, 1):\n",
    "        filename = os.path.basename(doc_path)\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ðŸ“„ Document {idx}/{len(document_paths)}: {filename}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Check if already processed\n",
    "        if state_manager.is_processed(doc_path):\n",
    "            print(\"â­ï¸  Already processed (file hash matches) - SKIPPING\")\n",
    "            skipped_count += 1\n",
    "            consecutive_failures = 0  # Reset on skip\n",
    "            continue\n",
    "        \n",
    "        # Process document\n",
    "        try:\n",
    "            result = await analyze_single_document(doc_path)\n",
    "            \n",
    "            if result:\n",
    "                # Mark as successfully processed\n",
    "                state_manager.mark_processed(\n",
    "                    file_path=doc_path,\n",
    "                    summary_path=result.get(\"summary_path\"),\n",
    "                    json_path=result.get(\"json_path\"),\n",
    "                    status=\"success\"\n",
    "                )\n",
    "                processed_count += 1\n",
    "                consecutive_failures = 0\n",
    "                print(f\"âœ… Successfully processed and saved\")\n",
    "            else:\n",
    "                # Processing returned None (error occurred)\n",
    "                state_manager.mark_failed(doc_path, \"Processing returned no result\")\n",
    "                failed_count += 1\n",
    "                consecutive_failures += 1\n",
    "                print(f\"âŒ Processing failed\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            error_msg = f\"{type(e).__name__}: {str(e)}\"\n",
    "            print(f\"âŒ Error: {error_msg}\")\n",
    "            state_manager.mark_failed(doc_path, error_msg)\n",
    "            failed_count += 1\n",
    "            consecutive_failures += 1\n",
    "        \n",
    "        # Check if too many consecutive failures\n",
    "        if consecutive_failures >= max_failures:\n",
    "            print(f\"\\nâš ï¸  Stopping batch: {consecutive_failures} consecutive failures\")\n",
    "            break\n",
    "        \n",
    "        # Progress update\n",
    "        print(f\"\\nðŸ“Š Batch Progress: {processed_count} processed | {skipped_count} skipped | {failed_count} failed\")\n",
    "    \n",
    "    # Final summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ“Š BATCH PROCESSING COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"âœ… Successfully processed: {processed_count}\")\n",
    "    print(f\"â­ï¸  Skipped (already done): {skipped_count}\")\n",
    "    print(f\"âŒ Failed: {failed_count}\")\n",
    "    print(f\"ðŸ“ Total attempted: {processed_count + failed_count}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return {\n",
    "        \"processed\": processed_count,\n",
    "        \"skipped\": skipped_count,\n",
    "        \"failed\": failed_count\n",
    "    }\n",
    "\n",
    "\n",
    "async def analyze_single_document(document_path: str):\n",
    "    \"\"\"\n",
    "    Analyze a single document using GPT-5's multimodal capabilities.\n",
    "    Modified version of analyze_document_multimodal for batch processing.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with summary, extracted_data, and output paths, or None on failure\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get timestamps\n",
    "        analysis_date = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        analysis_date_prefix = datetime.now().strftime('%Y%m%d')\n",
    "        \n",
    "        # Extract filename info\n",
    "        filename = Path(document_path).name\n",
    "        filename_without_ext = Path(document_path).stem\n",
    "        \n",
    "        # Define output paths\n",
    "        summary_output_dir = \"/lakehouse/default/Files/Document_Summaries\"\n",
    "        json_output_dir = \"/lakehouse/default/Files/Document_Analysis\"\n",
    "        \n",
    "        summary_output_path = f\"{summary_output_dir}/{analysis_date_prefix}_{filename_without_ext}.md\"\n",
    "        json_output_path = f\"{json_output_dir}/{analysis_date_prefix}_{filename_without_ext}.json\"\n",
    "        \n",
    "        # Create output directories\n",
    "        os.makedirs(summary_output_dir, exist_ok=True)\n",
    "        os.makedirs(json_output_dir, exist_ok=True)\n",
    "        \n",
    "        print(f\"ðŸ“ Input: {filename}\")\n",
    "        \n",
    "        # Step 1: Process document\n",
    "        print(\"Step 1: Processing document...\")\n",
    "        document_images, text_content = document_to_images(document_path)\n",
    "        \n",
    "        if document_images and len(document_images) > 0:\n",
    "            has_images = True\n",
    "            text_content = None\n",
    "        elif text_content:\n",
    "            has_images = False\n",
    "            document_images = None\n",
    "        else:\n",
    "            # Try fallback for plain text files\n",
    "            file_extension = Path(document_path).suffix.lower()\n",
    "            if file_extension in ['.txt', '.md']:\n",
    "                with open(document_path, 'r', encoding='utf-8') as f:\n",
    "                    text_content = f.read()\n",
    "                has_images = False\n",
    "                document_images = None\n",
    "            else:\n",
    "                raise ValueError(f\"Could not process file: {document_path}\")\n",
    "        \n",
    "        # Step 2: Create analyzer agent\n",
    "        print(\"Step 2: Creating analyzer agent...\")\n",
    "        analyzer_agent = FabricAgent(\n",
    "            deployment_name=DEPLOYMENT_NAME,\n",
    "            api_version=API_VERSION,\n",
    "            instructions=DOCUMENT_ANALYZER_PROMPT,\n",
    "            name=\"DocumentAnalyzer\"\n",
    "        )\n",
    "        \n",
    "        # Step 3: Analyze document\n",
    "        print(\"Step 3: Analyzing with GPT-5...\")\n",
    "        if has_images:\n",
    "            total_size_kb = sum(len(img) for img in document_images) / 1024\n",
    "            \n",
    "            if total_size_kb > 3000 or len(document_images) > 10:\n",
    "                # Process in chunks\n",
    "                summaries = []\n",
    "                chunk_size = 5\n",
    "                for i in range(0, len(document_images), chunk_size):\n",
    "                    chunk = document_images[i:i+chunk_size]\n",
    "                    chunk_result = await analyzer_agent.run(\n",
    "                        user_message=f\"Analyze pages {i+1}-{i+len(chunk)} of this document. Provide key findings.\",\n",
    "                        images=chunk\n",
    "                    )\n",
    "                    summaries.append(chunk_result.text)\n",
    "                \n",
    "                combined_summary = \"\\n\".join(summaries)\n",
    "                final_result = await analyzer_agent.run(\n",
    "                    user_message=f\"Synthesize these summaries into one comprehensive executive summary:\\n\\n{combined_summary}\"\n",
    "                )\n",
    "                summary = final_result.text\n",
    "            else:\n",
    "                analyzer_result = await analyzer_agent.run(\n",
    "                    user_message=\"Please analyze this document and provide a comprehensive executive summary.\",\n",
    "                    images=document_images\n",
    "                )\n",
    "                summary = analyzer_result.text\n",
    "        else:\n",
    "            if len(text_content) > 100000:\n",
    "                # Chunk large text\n",
    "                chunk_size = 50000\n",
    "                summaries = []\n",
    "                for i in range(0, len(text_content), chunk_size):\n",
    "                    chunk = text_content[i:i+chunk_size]\n",
    "                    chunk_result = await analyzer_agent.run(\n",
    "                        user_message=f\"Analyze this section of the document:\\n\\n{chunk}\"\n",
    "                    )\n",
    "                    summaries.append(chunk_result.text)\n",
    "                \n",
    "                combined_summary = \"\\n\\n\".join(summaries)\n",
    "                final_result = await analyzer_agent.run(\n",
    "                    user_message=f\"Synthesize these summaries:\\n\\n{combined_summary}\"\n",
    "                )\n",
    "                summary = final_result.text\n",
    "            else:\n",
    "                analyzer_result = await analyzer_agent.run(\n",
    "                    user_message=f\"Please analyze this document and provide a comprehensive executive summary:\\n\\n{text_content}\"\n",
    "                )\n",
    "                summary = analyzer_result.text\n",
    "        \n",
    "        # Step 4: Create extractor agent\n",
    "        print(\"Step 4: Extracting structured data...\")\n",
    "        extractor_agent = FabricAgent(\n",
    "            deployment_name=DEPLOYMENT_NAME,\n",
    "            api_version=API_VERSION,\n",
    "            instructions=JSON_EXTRACTOR_PROMPT,\n",
    "            name=\"JSONExtractor\"\n",
    "        )\n",
    "        \n",
    "        # Step 5: Extract data\n",
    "        if has_images:\n",
    "            extractor_result = await extractor_agent.run(\n",
    "                user_message=\"Extract structured information from this document and return as JSON.\",\n",
    "                images=document_images\n",
    "            )\n",
    "        else:\n",
    "            extractor_result = await extractor_agent.run(\n",
    "                user_message=f\"Extract structured information from this document and return as JSON:\\n\\n{text_content}\"\n",
    "            )\n",
    "        \n",
    "        # Parse JSON\n",
    "        json_text = extractor_result.text.strip()\n",
    "        if json_text.startswith(\"```\"):\n",
    "            json_text = json_text.split(\"```\")[1]\n",
    "            if json_text.startswith(\"json\"):\n",
    "                json_text = json_text[4:]\n",
    "            json_text = json_text.strip()\n",
    "        \n",
    "        try:\n",
    "            extracted_data = json.loads(json_text)\n",
    "        except json.JSONDecodeError:\n",
    "            extracted_data = {\"raw_response\": extractor_result.text}\n",
    "        \n",
    "        # Step 6: Save outputs\n",
    "        print(\"Step 5: Saving outputs...\")\n",
    "        \n",
    "        # Save markdown\n",
    "        with open(summary_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"# DOCUMENT SUMMARY\\n\\n\")\n",
    "            f.write(\"---\\n\\n\")\n",
    "            f.write(f\"**Source Document:** {filename}  \\n\")\n",
    "            f.write(f\"**Analysis Date:** {analysis_date}  \\n\")\n",
    "            f.write(f\"**Model:** {DEPLOYMENT_NAME} (Multimodal)  \\n\")\n",
    "            f.write(f\"**Approach:** Two-Agent Workflow (Analyzer + Extractor)  \\n\\n\")\n",
    "            f.write(\"---\\n\\n\")\n",
    "            f.write(summary)\n",
    "            f.write(\"\\n\\n---\\n\")\n",
    "        \n",
    "        # Save JSON\n",
    "        from urllib.parse import quote\n",
    "        document_filename = Path(document_path).name\n",
    "        sharepoint_doc_url = f\"{SHAREPOINT_DOCUMENT_ROOT}{quote(document_filename)}\"\n",
    "        \n",
    "        full_json_data = {\n",
    "            \"source_document\": filename,\n",
    "            \"document_location\": sharepoint_doc_url,\n",
    "            \"analysis_date\": analysis_date,\n",
    "            \"analysis_metadata\": {\n",
    "                \"document_path\": document_path,\n",
    "                \"analysis_model\": DEPLOYMENT_NAME,\n",
    "                \"api_version\": API_VERSION,\n",
    "                \"authentication\": \"Fabric Managed Identity\",\n",
    "                \"approach\": \"Multimodal GPT-5 Vision\",\n",
    "                \"agents_used\": [\"DocumentAnalyzer\", \"JSONExtractor\"],\n",
    "                \"has_images\": has_images,\n",
    "                \"image_count\": len(document_images) if has_images else 0\n",
    "            },\n",
    "            \"summary_text\": summary,\n",
    "            \"extracted_data\": extracted_data\n",
    "        }\n",
    "        \n",
    "        with open(json_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(full_json_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"ðŸ’¾ Saved: {os.path.basename(summary_output_path)}\")\n",
    "        print(f\"ðŸ’¾ Saved: {os.path.basename(json_output_path)}\")\n",
    "        \n",
    "        return {\n",
    "            \"summary\": summary,\n",
    "            \"extracted_data\": extracted_data,\n",
    "            \"document_location\": sharepoint_doc_url,\n",
    "            \"summary_path\": summary_output_path,\n",
    "            \"json_path\": json_output_path\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error processing document: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"âœ… Batch processing functions defined\")\n",
    "print(\"ðŸ’¡ Ready to process multiple documents with state tracking!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5af1a36",
   "metadata": {},
   "source": [
    "### 7.2 Execute Batch Processing\n",
    "\n",
    "Run automated processing on all unprocessed documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6d2ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== BATCH PROCESSING EXECUTION =====================\n",
    "\n",
    "# Get unprocessed documents\n",
    "unprocessed = get_unprocessed_documents(search_pattern=\"Files/SharePointDocuments/**/*\")\n",
    "\n",
    "if not unprocessed:\n",
    "    print(\"\\nâœ… No new documents to process!\")\n",
    "    print(\"ðŸ’¡ All documents in the Files directory have been analyzed.\")\n",
    "else:\n",
    "    print(f\"\\nðŸš€ Starting batch processing of {len(unprocessed)} document(s)...\")\n",
    "    \n",
    "    # Run batch processing\n",
    "    result = await process_document_batch(unprocessed, max_failures=5)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸŽ‰ BATCH PROCESSING FINISHED!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ðŸ“Š Total state:\")\n",
    "    print(f\"   âœ… All-time processed: {state_manager.get_processed_count()}\")\n",
    "    print(f\"   âŒ All-time failed: {state_manager.get_failed_count()}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Optional: Show failed documents\n",
    "    if result[\"failed\"] > 0:\n",
    "        print(\"\\nâš ï¸  Failed documents (check state file for details):\")\n",
    "        for file_name, info in state_manager.state[\"processed_files\"].items():\n",
    "            if info.get(\"status\") == \"failed\":\n",
    "                print(f\"   - {file_name}: {info.get('error', 'Unknown error')}\")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "a16fa217-51e1-4e49-9da6-c0de10340069",
    "default_lakehouse_name": "FabricAI_LH",
    "default_lakehouse_workspace_id": "cbf3b651-7ffe-41db-8698-70aa092c1d24",
    "known_lakehouses": [
     {
      "id": "a16fa217-51e1-4e49-9da6-c0de10340069"
     }
    ]
   }
  },
  "kernel_info": {
   "jupyter_kernel_name": "python3.11",
   "name": "jupyter"
  },
  "kernelspec": {
   "display_name": "Jupyter",
   "name": "jupyter"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "jupyter_python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
